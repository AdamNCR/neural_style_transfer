{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font color=\"orange\">Style Transfer</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><u>Background</u></h2></center>\n",
    "<br>\n",
    "\n",
    "<h4>What is Style Transfer?</h4>\n",
    "<p>Style transfer is the technique of recomposing images in the style of other images. For instance, we could take a picture of the Mona Lisa and paint it in a different style:</p>\n",
    "<img src=\"notebook_images/style_transfer_example.png\"/>\n",
    "\n",
    "<h4>The Paper:</h4>\n",
    "<p>The idea came about by Leon A. Gatys et al. who plublished <i><a href=\"https://arxiv.org/pdf/1508.06576.pdf\">A Neural Algorithm of Artistic Style</a></i> in 2015. They proposed style transfer as an optimization problem which can be solved through training a neural network. Specifically, the authors use a pre-trained convolutional neural network, called the VGG16 model, that has already developed intuition for some internal representation of content and style within a given image.</p>\n",
    "<br>\n",
    "\n",
    "<center><h2><u>Prerequisites</u></h2></center>\n",
    "<br>\n",
    "\n",
    "<h4>Neural Network:</h4>\n",
    "<p>A specific model within the field of deep learning that attempts to mimic how the human brain process information. The neural network moves input data forward through a series layers and calculating weights to create a prediction. The prediction is then compared to the actual value and the network moves backwards to update the weights. The network produces a function $f$ that takes in an input $x$ and produces an output $y$ such that $f(x) = y$.</p>\n",
    "\n",
    "<h4>Convolutional Layers:</h4>\n",
    "<p>Convolution is a mathematical operation that takes a kernel, a fixed square window of weights, and sums the pixel values of the image within the kernel window multiplied by the corresponding kernel weight. Convolution tries to detect the presence of a feature within the kernel window and produces a new window of numbers called a feature map. For example:</p>\n",
    "<img src=\"notebook_images/convolution_1.png\"/>\n",
    "<img src=\"notebook_images/convolution_2.gif\"/>\n",
    "<p>Convolutional layers in neural networks are industry standard for performing tasks such as image classification. The feature maps give us some intuition about what is in the content of an image.</p>\n",
    "\n",
    "<h4>Fully Connected Layers:</h4>\n",
    "<p>A fully connected layer takes an input and multiplies it by a weight matrix and then adds a bias vector.</p>\n",
    "<img src=\"notebook_images/fully_connected_layer.png\" style=\"width:300px;height:350px\"/>\n",
    "<p>At the end of a convolutional neural network it is common to find a series of fully connected layers. The output in a convolutional layer represents high-level features in the data. Adding a fully connected layer is an easy and cheap way of learning non-linear combinations of these features. Essentially the convolutional layers are providing a meaningful, low-dimensional, and somewhat invariant feature space, and the fully-connected layer is learning a (possibly non-linear) function in that space.</p>\n",
    "\n",
    "\n",
    "<h4>Rectified Linear Units (ReLus):</h4>\n",
    "<p>A ReLu is an activation function that computes the function $f(x) = max(0,x)$. The function is quite simple and is frequently used in non-output layers as it speeds up training. The computational step of a ReLu is easy; any negative elements are set to 0.0 and there are no exponentials, no multiplication, and no division operations. ReLu is often computed after the convolution operation and serves as a nonlinear activation function like a hyperbolic tangent (tanh) or sigmoid would be. The ReLu function looks as follows:</p>\n",
    "<img src=\"notebook_images/relu.jpeg\"/>\n",
    "\n",
    "<h4>Max Pooling:</h4>\n",
    "<p>Max Pooling is a down-sampling operation on an input representation, it reduces dimensionality and allows for assumptions to be made about features contained in a binned sub-region. The process is as follows:</p>\n",
    "<img src=\"notebook_images/max_pooling.png\"/>\n",
    "<p>Max pooling allows some sense of translational invariance. For instance, max pooling allows us to learn that a face has the following components: eyes, nose, lips, and ears. Translational invariance means that as long as these components are present in an image, it does not matter what arangement the components are in. For example: </p> \n",
    "<img src=\"notebook_images/max_pooling_2.png\"/>\n",
    "<p>Above, both images would be classified as a face in a network that uses maxpooling as the components for a face are present in both images.</p>\n",
    "\n",
    "\n",
    "<h4>VGG Network:</h4>\n",
    "<p>ImageNet is a large scale visual recognition challenge where teams compete to classify millions of images with objects that come from almost 1,000 categories. In 2014, the Visual Geometry Group (VGG) from Oxford University, won this challenge with a classification error rate of only 7.0%. Gatys et. al used this pretrained network for their application of style transfer. The network consists of 16 layers of convolution and ReLU non-linearity, separated by 5 pooling layers and ending in 3 fully connected layers. The architecture is as follows:</p>\n",
    "<img src=\"notebook_images/vgg_architecture.png\"/>\n",
    "\n",
    "<h4>Content Representation:</h4>\n",
    "<p>Content is defined as what is in the image. For instance, if we had a picture of the Mona Lisa, the face and body of the painting would be the content. Networks that have been trained for the task of object recognition learn which features it is important to extract from an image in order to identify its content. The feature maps in the convolution layers of the network can be seen as the network's internal representation of the image content. Thus, we can represent the content of an image by extracting the feature map from the convolutional layers in a neural network.</p>\n",
    "\n",
    "<h4>Style Representation:</h4>\n",
    "<p>Style of an image is defined as the way in which the content is presented. For instance, the color and swirly nature in the Starry Night painting represents the style of the paining. The style of an image is not well captured by simply looking at the values of a feature map from the convolutional layer in the neural network. However, Gatys et. al found that we can extract a style representation by looking at the spatial correlation of the values within the feature map. To do this, the Gram matrix of the feature map is calculated. If we have a feature map $F$ represented as a matrix then the Gram matrix $G$ can be calculated with each entry $G_{ij} = \\sum_{k}F_{ik}F_{jk}$. Using this, we can see that if we had two images whose feature maps at a given layer produced the same Gram matrix then we would find that the two images had similar style, but not necessairly similar content. Gram et. al found that the best results for style representation came from applying the gram matrix to a combination of shallow and deep layers. Applying this to early layers in the network would capture some of the finer textures contained within the image whereas applying this to deeper layers would capture more higher-level elements of the imageâ€™s style. </p>\n",
    "\n",
    "<h4>Optimization:</h4>\n",
    "<p>With style transfer we have the task of generating a new image $Y$, whose style is equal to a style image $S$ and whose content is equal to a content image $C$. Thus, we have to define a loss for our content and style. Let $l$ be a chosen content layer, then the content loss is defined as the euclidean distance between the feature map $F^{l}$ of our content image $C$ and the feature map $P^{l}$ of our generated image $Y$. We define the content loss $$L_{\\text{content}} = \\frac{1}{2}\\sum_{ij}\\left(F_{ij}^l - P_{ij}^l\\right)^2$$.\n",
    "For a chosen style layer $l$, the style loss is defined as the euclidean distance between the Gram matrix $G^l$ of the feature map of our style image $S$ and the Gram matrix $A^l$ of the feature map of our generated image $Y$. We define the style loss $$L_{\\text{style}} = \\frac{1}{2}\\sum_{l=0}^L\\left(G_{ij}^l - A_{ij}^l\\right)^2.$$\n",
    "We can now take a weighted sum of the style and content losses, where we can adjust weights to preserve more of the style or more of the content. The total loss $$l_{\\text{total}} = \\alpha L_{\\text{content}} + \\beta L_{\\text{style}}$$\n",
    "where $\\alpha, \\beta \\in \\mathbb{R}$.We now can perform the task of style transfer by trying to generate an image $Y$ which can minimize the loss function $L_{\\text{total}}$.\n",
    "</p>\n",
    "\n",
    "<h4>The Full Pipeline:</h4>\n",
    "<p>The pipeline of the style transfer can be depicted visually as follows:\n",
    "</p>\n",
    "<img src=\"notebook_images/style_transfer_pipeline.gif\"/>\n",
    "<p>Above we use forward and backward propogation to minimize our loss.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><u>Code</u></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "\n",
    "#Data Handling\n",
    "import numpy as np\n",
    "\n",
    "#Image Handling \n",
    "from PIL import Image\n",
    "\n",
    "#Deep Learning \n",
    "from keras import backend as K \n",
    "from keras.applications import vgg16\n",
    "from keras.applications.vgg16 import preprocess_input \n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import VGG16\n",
    "\n",
    "#Optimization\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 1: Load and Preprocess Images</h4>\n",
    "<p>The first step is to load the content and style images and preprocess them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessImage(content_image,style_image,targetHeight,targetWidth):  \n",
    "    target_shape = (targetHeight,targetWidth)\n",
    "    \n",
    "    #Load the content image\n",
    "    cImage = load_img(path=content_image,target_size=target_shape)\n",
    "    cImageArr = img_to_array(cImage)\n",
    "    cImageArr = K.variable(preprocess_input(np.expand_dims(cImageArr,axis=0)), dtype=\"float32\")\n",
    "    \n",
    "    #Load the style image\n",
    "    sImage = load_img(path=style_image,target_size=target_shape)\n",
    "    sImageArr = img_to_array(cImage)\n",
    "    sImageArr = K.variable(preprocess_input(np.expand_dims(sImageArr,axis=0)), dtype=\"float32\")\n",
    "    \n",
    "    #Generate some noise \n",
    "    gImage = np.random.randint(256, size=(targetWidth, targetHeight, 3)).astype('float64')\n",
    "    gImage = preprocess_input(np.expand_dims(gImage, axis=0))\n",
    "    gImagePlaceholder = K.placeholder(shape=(1, targetWidth, targetHeight, 3))\n",
    "    \n",
    "    return cImageArr, sImageArr, gImage, gImagePlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 2: Get the Feature Representation:</h4>\n",
    "<p>Here we get the feature representation for an input $x$ for one or more layers in our model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureRepresentation(x,layer_names,model): \n",
    "    featMatrices = []\n",
    "    for ln in layer_names:\n",
    "        selectedLayer = model.get_layer(ln)\n",
    "        featRaw = selectedLayer.output\n",
    "        featRawShape = K.shape(featRaw).eval(session=tf_session)\n",
    "        N_l = featRawShape[-1]\n",
    "        M_l = featRawShape[1]*featRawShape[2]\n",
    "        featMatrix = K.reshape(featRaw, (M_l, N_l))\n",
    "        featMatrix = K.transpose(featMatrix)\n",
    "        featMatrices.append(featMatrix)\n",
    "    return featMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 3: Get the Content Loss:</h4>\n",
    "<p>We need to return the content loss $$L_{\\text{content}} = \\frac{1}{2}\\sum_{ij}\\left(F_{ij}^l - P_{ij}^l\\right)^2$$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContentLoss(F, P):\n",
    "    L_content = 0.5*K.sum(K.square(F - P))\n",
    "    return L_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 4: Get the Gram Matrix:</h4>\n",
    "<p>We need to calculate the Gram matrix for our style.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGramMatrix(F): \n",
    "    gram_matrix = K.dot(F,K.transpose(F))\n",
    "    return gram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 5: Get the Style Loss:</h4>\n",
    "<p>We need to return the style loss $$L_{\\text{style}} = \\frac{1}{2}\\sum_{l=0}^L\\left(G_{ij}^l - A_{ij}^l\\right)^2$$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStyleLoss(ws, Gs, As):\n",
    "    sLoss = K.variable(0.)\n",
    "    for w, G, A in zip(ws, Gs, As):\n",
    "        M_l = K.int_shape(G)[1]\n",
    "        N_l = K.int_shape(G)[0]\n",
    "        G_gram = getGramMatrix(G)\n",
    "        A_gram = getGramMatrix(A)\n",
    "        sLoss+= w*0.25*K.sum(K.square(G_gram - A_gram))/ (N_l**2 * M_l**2)\n",
    "    return sLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 6: Get the total loss:</h4>\n",
    "<p>We need to return the total loss as $$l_{\\text{total}} = \\alpha L_{\\text{content}} + \\beta L_{\\text{style}}$$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTotalLoss(gImagePlaceholder,alpha=1.0,beta=1000.0): \n",
    "    F = getFeatureRepresentation(gImagePlaceholder, layer_names=[cLayerName], model=gModel)[0]\n",
    "    Gs = getFeatureRepresentation(gImagePlaceholder, layer_names=sLayerNames, model=gModel)\n",
    "    contentLoss = getContentLoss(F, P)\n",
    "    styleLoss = getStyleLoss(ws, Gs, As)\n",
    "    l_total = alpha*contentLoss + beta*styleLoss\n",
    "    return l_total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 7: Minimization of Loss:</h4> \n",
    "<p>Calculate total loss from our generated image to minimize the loss function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(gImArr):\n",
    "    if gImArr.shape != (1, targetWidth, targetWidth, 3):\n",
    "        gImArr = gImArr.reshape((1, targetWidth, targetHeight, 3))\n",
    "    loss_fcn = K.function([gModel.input], [getTotalLoss(gModel.input)])\n",
    "    return loss_fcn([gImArr])[0].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 8: Calculate Gradient of Loss:</h4>\n",
    "<p>Calcualte the gradient of the loss function with respect to the generated image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grad(gImArr):\n",
    "    if gImArr.shape != (1, targetWidth, targetHeight, 3):\n",
    "        gImArr = gImArr.reshape((1, targetWidth, targetHeight, 3))\n",
    "    grad_fcn = K.function([gModel.input], \n",
    "                          K.gradients(getTotalLoss(gModel.input), [gModel.input]))\n",
    "    grad = grad_fcn([gImArr])[0].flatten().astype('float64')\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 9: Instantiate Model:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Paths for content and style images \n",
    "content_image_path = \"content_images/tech_tower.jpg\"\n",
    "style_image_path = \"style_images/starry_night_van_goh.jpg\"\n",
    "\n",
    "#Content Image Properties \n",
    "cImageOrig = Image.open(content_image_path)\n",
    "cImageSizeOrig = cImageOrig.size\n",
    "\n",
    "#Path for output image \n",
    "genImOutputPath = \"results/starry_tech_tower.jpg\"\n",
    "\n",
    "#Target height/width for images\n",
    "targetHeight = 512 \n",
    "targetWidth = 512\n",
    "cImageArr, sImageArr, gImage, gImPlaceholder = preprocessImage(content_image_path,\n",
    "                                            style_image_path,targetHeight,targetWidth)\n",
    "\n",
    "\n",
    "#Setup Session \n",
    "tf_session = K.get_session()\n",
    "\n",
    "#Select Models\n",
    "cModel = VGG16(include_top=False, weights='imagenet', input_tensor=cImageArr)\n",
    "sModel = VGG16(include_top=False, weights='imagenet', input_tensor=sImageArr)\n",
    "gModel = VGG16(include_top=False, weights='imagenet', input_tensor=gImPlaceholder)\n",
    "\n",
    "#Select layers for content and style\n",
    "cLayerName = 'block4_conv2'\n",
    "sLayerNames = [\n",
    "                'block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                ]\n",
    "\n",
    "#Get initial feature representations\n",
    "P = getFeatureRepresentation(x=cImageArr, layer_names=[cLayerName], model=cModel)\n",
    "P = P[0]\n",
    "As = getFeatureRepresentation(x=sImageArr,layer_names=sLayerNames,model=sModel)\n",
    "ws = np.ones(len(sLayerNames))/float(len(sLayerNames))\n",
    "\n",
    "#Number of training epochs\n",
    "iterations = 500 \n",
    "\n",
    "#Minimize the loss using the L-BFGS-B algorithm\n",
    "x_val = gImage.flatten()\n",
    "xopt, f_val, info= fmin_l_bfgs_b(calculate_loss, x_val, fprime=get_grad,\n",
    "                            maxiter=iterations, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 10: View Results:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess_array(x):\n",
    "    # Zero-center by mean pixel\n",
    "    if x.shape != (targetWidth, targetHeight, 3):\n",
    "        x = x.reshape((targetWidth, targetHeight, 3))\n",
    "    x[..., 0] += 103.939\n",
    "    x[..., 1] += 116.779\n",
    "    x[..., 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[..., ::-1]\n",
    "    x = np.clip(x, 0, 255)\n",
    "    x = x.astype('uint8')\n",
    "    return x\n",
    "\n",
    "def reprocess_array(x):\n",
    "    x = np.expand_dims(x.astype('float64'), axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def save_original_size(x, target_size=cImageSizeOrig):\n",
    "    xIm = Image.fromarray(x)\n",
    "    xIm = xIm.resize(target_size)\n",
    "    xIm.save(genImOutputPath)\n",
    "    return xIm\n",
    "\n",
    "\n",
    "xOut = postprocess_array(xopt)\n",
    "xIm = save_original_size(xOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#View Generated Image \n",
    "plt.imshow(xOut)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tips:</h4>\n",
    "<ul>\n",
    "<li><b>Different Weightings:</b> Try setting $\\alpha$ and $\\beta$ to different values. Typically we want $\\frac{\\beta}{\\alpha} \\sim 10^5$</li>\n",
    "<li><b>More Style Layers:</b> Add more style layers into sLayerNames, this will be computationally more expensive though.</li>\n",
    "<li><b>More Content Layers:</b> Try adding more content layers.</li>\n",
    "<li><b>Different Model Architectures:</b> Try using different architectures like VGG19, ResNet, or Inception instead of VGG16.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bonus:</h4>\n",
    "<p>The Neural Style Transfer model uses the VGG16 architecture. This architecture can be built in keras as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "from keras.models import port Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define an input shape\n",
    "inpt_shpe = (150,150,3)\n",
    "output_dim = 10\n",
    "#Define the number of classes to output in the multi-class classification problem\n",
    "def VGG16(inpt_shpe,output_dim): \n",
    "    \n",
    "    #Initialize Model\n",
    "    model = Sequential()\n",
    "    #model.add(InputLayer(input_shape=inpt_shpe))\n",
    "    model.add(Input(inpt_shape=inpt_shpe))\n",
    "    \n",
    "    #Block 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Block 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Block 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Block 4\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Block 5\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_maxpool'))\n",
    "    \n",
    "    #Classification Block \n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(4096,activation='relu',name='fc1'))\n",
    "    model.add(Dense(4096,activation='relu',name='fc2'))\n",
    "    model.add(Dense(output_dim,activation='softmax',name='predictions'))\n",
    "    \n",
    "    #Compile model with stochastic gradient descent optimizer\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
